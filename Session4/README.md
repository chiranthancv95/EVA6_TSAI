## EVA6 Session4 Assignment

This assignment has been divided into two parts - 
Part 1 and Part 2

The following is the problem statement for the two parts.

## Assignment:<br>
### PART 1[250]: Rewrite the whole excel sheet showing backpropagation. Explain each major step, and write it on Github.
Use [0.3, 0.5, -0.2, 0.7, 0.1, -0.6, 0.3, -0.9] for [w1, w2, w3, w4, w5, w6, w7, w8] variables instead of the other variables used in the class <br>
Take a screenshot, and show that screenshot in the readme file<br>
Excel file must be there for us to cross-check the image shown on readme (no image = no score)<br>
Explain each major step<br>
Show what happens to the error graph when you change the learning rate from [0.1, 0.2, 0.5, 0.8, 1.0, 2.0]<br> 
Upload all this to GitHub and then write all above as part 1 of your README.md file. <br>
Submit details to S4 - Assignment QnA.<br>
### PART 2 [250]: We have considered many points in our last 4 lectures. Some of these we have covered directly and some indirectly. They are:<br>
How many layers,<br>
MaxPooling,<br>
1x1 Convolutions,<br>
3x3 Convolutions,<br>
Receptive Field,<br>
SoftMax,<br>
Learning Rate,<br>
Kernels and how do we decide the number of kernels?<br>
Batch Normalization,<br>
Image Normalization,<br>
Position of MaxPooling,<br>
Concept of Transition Layers,<br>
Position of Transition Layer,<br>
DropOut<br>
When do we introduce DropOut, or when do we know we have some overfitting<br>
The distance of MaxPooling from Prediction,<br>
The distance of Batch Normalization from Prediction,<br>
When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)<br>
How do we know our network is not going well, comparatively, very early<br>
Batch Size, and effects of batch size<br>
etc (you can add more if we missed it here)<br>
Refer to this code: COLABLINK<br>
WRITE IT AGAIN SUCH THAT IT ACHIEVES<br>
99.4% validation accuracy<br>
You have used parameters exactly between 12000 to 18000<br>
You can use anything from above you want. <br>
You have used exactly 19 epochs<br>
Have used BN, Dropout, a Fully connected layer, have used GAP. <br>
Your Dropout must have 0.069 as the dropout value<br>
Your batch size must be exactly 128<br>
You must add random rotation to your images between -5 to +5 degrees.<br>
To learn how to add different things we covered in this session, you can refer to this code: https://www.kaggle.com/enwei26/mnist-digits-pytorch-cnn-99 DONT COPY ARCHITECTURE, JUST LEARN HOW TO INTEGRATE THINGS LIKE DROPOUT, BATCHNORM, ETC.<br>
This is a slightly time-consuming assignment, please make sure you start early. You are going to spend a lot of effort into running the programs multiple times<br>
Once you are done, submit your results in S4-Assignment-Solution<br>
You must upload your assignment to a public GitHub Repository. Create a folder called S4 in it, and add your iPynb code to it. THE LOGS MUST BE VISIBLE.<br> 
Before adding the link to the submission make sure you have opened the file in an "incognito" window. <br>
